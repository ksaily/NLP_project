{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b388068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "#!pip install pandas\n",
    "#!pip install openpyxl\n",
    "#!pip install ndlib\n",
    "#!pip install wordcloud\n",
    "#!pip install empath\n",
    "#!pip install keras\n",
    "#!pip install theano\n",
    "#!pip install gensim\n",
    "#!pip install xgboost\n",
    "#!pip install NLTK\n",
    "#!pip install Sklearn\n",
    "#!pip install numpy\n",
    "#!pip install tweepy\n",
    "#!pip install tensorflow\n",
    "#!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tweepy\n",
    "import csv\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"AskFm_datasets.csv\", \",\", index_col=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd4899",
   "metadata": {},
   "source": [
    "# 1. Wordcloud representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "hs_text = \"\"\n",
    "cs_text = \"\"\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data[1][i] == \"0\":\n",
    "        cs_text += \" \" + data[0][i]\n",
    "    elif data[1][i] == \"1\":\n",
    "        hs_text += \" \" + data[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4110ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abusive content wordcloud representation\n",
    "wordcloud = WordCloud().generate(hs_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b18bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-abusive content wordcloud representation\n",
    "wordcloud = WordCloud().generate(cs_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b352fa",
   "metadata": {},
   "source": [
    "# 2. Empath categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "\n",
    "lexicon = Empath()\n",
    "\n",
    "abusive_categories = []\n",
    "non_abusive_categories = []\n",
    "\n",
    "# analyze abusive content data\n",
    "for i in range(1, len(data)):\n",
    "    if data[1][i] == \"1\":\n",
    "        temp = lexicon.analyze(data[0][i], normalize=True)\n",
    "        for j in temp.items():\n",
    "            if j[1] != 0.0:\n",
    "                if j[0] not in abusive_categories:\n",
    "                    abusive_categories.append(j[0])\n",
    "\n",
    "# analyze non-abusive content data\n",
    "for i in range(1, len(data)):\n",
    "    if data[1][i] == \"0\":\n",
    "        temp = lexicon.analyze(data[0][i], normalize=True)\n",
    "        for j in temp.items():\n",
    "            if j[1] != 0.0:\n",
    "                if j[0] not in non_abusive_categories:\n",
    "                    non_abusive_categories.append(j[0])\n",
    "            \n",
    "# combine categories into one list\n",
    "while abusive_categories:\n",
    "    x = abusive_categories.pop()\n",
    "    if x not in non_abusive_categories:\n",
    "        non_abusive_categories.append(x)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d14d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"MESSAGE\"] + [\"isAbusive\"] + non_abusive_categories\n",
    "\n",
    "#empath categorization results into csv file database\n",
    "def recordEmpath(filename):\n",
    "    with open(filename, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter = \";\")\n",
    "        writer.writerow(header)\n",
    "    \n",
    "        for i in range(1, len(data)):\n",
    "            temp = lexicon.analyze(data[0][i], normalize=True)\n",
    "            temp_data = [None] * len(header)\n",
    "            temp_data[0] = data[0][i]\n",
    "            temp_data[1] = data[1][i]\n",
    "            for j in temp.items():\n",
    "                if j[1] != 0.0:\n",
    "                    for z in range(len(header)):\n",
    "                        if j[0] == header[z]:\n",
    "                            temp_data[z] = j[1]\n",
    "            writer.writerow(temp_data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d26813",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordEmpath(\"empath_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa03a66",
   "metadata": {},
   "source": [
    "# 3. Harvard General Inquirer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d738bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inquirerData = pd.read_excel(\"inquirerbasic.xls\", index_col=None, header=None)\n",
    "\n",
    "def loopInquirer():\n",
    "    for j in range(1, len(inquirerData)): #WORDS FROM INQUIRER\n",
    "            wordToSearch = str(inquirerData[0][j])\n",
    "            if wordToSearch.endswith(suffixes):\n",
    "                wordToSearch = wordToSearch[:-2]\n",
    "            for word in msg_upper:\n",
    "                if wordToSearch == word: #word found from inquirerData\n",
    "                    for k in range(2, len(inquirerData.columns)-2): #Go through categories\n",
    "                        if inquirerData[k][j] != 0:\n",
    "                            temp_data[k] = inquirerData[k][j]\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228d1b5",
   "metadata": {},
   "source": [
    "THIS CAN TAKE UP TO 20 MINUTES!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e906c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ('#1','#2','#3','#4','#5','#6','#7')\n",
    "header = [\"MESSAGE\"] + [\"isAbusive\"]\n",
    "\n",
    "#Fill NaN values with 0\n",
    "inquirerData = inquirerData.fillna(0)\n",
    "\n",
    "for i in range(2, len(inquirerData.columns)-2):\n",
    "    header = header + [str(inquirerData[i][0])]\n",
    "\n",
    "#Harvard General Inquirer results into csv file database\n",
    "with open('inquirer_dataset.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter = \";\")\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for i in range(1, len(data)): #MESSAGES\n",
    "        msg = data[0][i].replace(\"?\",\"\").replace(\"!\",\"\").replace(\",\",\"\").replace(\".\",\"\").replace('\"',\"\").split(' ')\n",
    "        msg_upper = [x.upper() for x in msg]\n",
    "        temp_data = [None] * len(header)\n",
    "        temp_data[0] = data[0][i]\n",
    "        temp_data[1] = data[1][i]\n",
    "        temp_data = loopInquirer()\n",
    "        writer.writerow(temp_data)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4ce28",
   "metadata": {},
   "source": [
    "# 4. Simple heuristic to identify the presence of hate content using both empath categorization and general inquirer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35aead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordResults(name, abusiveMsgInquirer, abusiveMsgEmpath):\n",
    "    with open(name, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter = \";\")\n",
    "        header = [\"MESSAGE\"] + [\"isAbusiveOriginally\"] + [\"isAbusiveInquirer\"] + [\"isAbusiveEmpath\"] + [\"isAbusiveOR\"]\n",
    "        writer.writerow(header)\n",
    "    \n",
    "        for i in range(1, len(inquirer_data)):\n",
    "            temp_data = [None] * len(header)\n",
    "            temp_data[0] = inquirer_data[0][i]\n",
    "            temp_data[1] = inquirer_data[1][i]\n",
    "            temp_data[2] = abusiveMsgInquirer[i-1]\n",
    "            temp_data[3] = abusiveMsgEmpath[i-1]\n",
    "\n",
    "            #If inquirer OR empath categorization marked the msg as hate speech,\n",
    "            #mark it as hate speech\n",
    "            if (int(temp_data[2])== 1 or int(temp_data[3]) == 1):\n",
    "                temp_data[4] = 1\n",
    "            else:\n",
    "                temp_data[4] = 0\n",
    "            writer.writerow(temp_data)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inquirer_data = pd.read_csv(\"inquirer_dataset.csv\", \";\", index_col=None, header=None, low_memory=False)\n",
    "empath_data = pd.read_csv(\"empath_dataset.csv\", \";\", index_col=None, header=None, low_memory=False)\n",
    "\n",
    "abusiveMessagesInquirer = []\n",
    "abusiveMessagesEmpath = []\n",
    "\n",
    "# Loop all inquirer abusive messages\n",
    "for i in range(1, len(inquirer_data)): # Messages\n",
    "    \n",
    "    #Categories that can be related to hate speech\n",
    "    if inquirer_data[2][i] != \"Positiv\" and (inquirer_data[3][i] == \"Negativ\" or inquirer_data[15][i] == \"Pain\" or inquirer_data[33][i] == \"Relig\" or inquirer_data[39][i] == \"Race\"  or \n",
    "     (inquirer_data[42][i]  == \"Female\" and inquirer_data[3][i] == \"Negativ\") or \n",
    "     (inquirer_data[60][i] == \"BodyPt\" and inquirer_data[3][i] == \"Negativ\")):\n",
    "        #Negate\n",
    "        if inquirer_data[114][i] == \"Negate\":\n",
    "            abusiveMessagesInquirer.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            abusiveMessagesInquirer.append(1)\n",
    "            continue\n",
    "    else:\n",
    "        abusiveMessagesInquirer.append(0)\n",
    "        \n",
    "# Loop all empath abusive messages\n",
    "for i in range(1, len(empath_data)):\n",
    "    \n",
    "    #Mark as hate speech if the message has no positive_emotion(10) and\n",
    "    #has words from categories that could include hate speech\n",
    "    \n",
    "    #Categories that can be related to hate speech: \n",
    "    #violence (6), swearing_terms (11), negative_emotion (30)\n",
    "    #shape_and_size (32), body(33), hate (121)\n",
    "    if not float(empath_data[10][i]) > 0.0 and (float(empath_data[6][i]) > 0.0 or float(empath_data[11][i]) > 0.0 or float(empath_data[30][i]) > 0.0 or float(empath_data[32][i]) > 0.0 or float(empath_data[33][i]) > 0.0 or float(empath_data[121][i]) > 0.0):\n",
    "        abusiveMessagesEmpath.append(1)\n",
    "    else:\n",
    "        abusiveMessagesEmpath.append(0)\n",
    "        continue\n",
    "        \n",
    "recordResults(\"simpleheuristic.csv\", abusiveMessagesInquirer, abusiveMessagesEmpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7cf9a",
   "metadata": {},
   "source": [
    "# 5. Analyzing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdcfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeData(filename):\n",
    "    compareData = pd.read_csv(filename, \";\", index_col=None, header=None, low_memory=False)\n",
    "    match = 0\n",
    "    matchOnes = 0\n",
    "    matchZeros = 0\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "    falsePos = 0\n",
    "    falseNeg = 0\n",
    "\n",
    "    for i in range(1, len(compareData)):\n",
    "        if int(compareData[1][i]) == 1 and int(compareData[4][i]) == 1:\n",
    "            match = match + 1\n",
    "            matchOnes = matchOnes + 1\n",
    "            ones = ones + 1\n",
    "        elif int(compareData[1][i]) == 0 and int(compareData[4][i]) == 0:\n",
    "            match = match + 1\n",
    "            matchZeros = matchZeros + 1\n",
    "            zeros = zeros + 1\n",
    "        elif int(compareData[1][i]) == 0 and int(compareData[4][i]) == 1:\n",
    "            falsePos = falsePos + 1\n",
    "            zeros = zeros + 1\n",
    "        elif int(compareData[1][i]) == 1 and int(compareData[4][i]) == 0:\n",
    "            falseNeg = falseNeg + 1\n",
    "            ones = ones + 1\n",
    "\n",
    "    print(match, \"out of\", len(compareData)-1, \"posts have the same outcome\")\n",
    "    print(\"Complete Accuracy: \", round(match/(len(compareData)-1), 3)*100, \"%\")\n",
    "    print(falsePos, \"false positives out of\", zeros)\n",
    "    print(falseNeg, \"false negatives out of\", ones)\n",
    "    print(matchOnes, \"real positives out of\", ones)\n",
    "    print(matchZeros, \"real negatives out of\", zeros)\n",
    "    print(\"\\nPercent of hate speech recognized: \", round((matchOnes/ones)*100,2), \"%\")\n",
    "    print(\"Percent of real positives from all positives: \", round((matchOnes/(falsePos+matchOnes))*100,2), \"%\")\n",
    "    \n",
    "    \n",
    "    #return Complete Accuracy\n",
    "    return(str(round(match/(len(compareData)-1), 3)*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeData(\"simpleheuristic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dfbf79",
   "metadata": {},
   "source": [
    "# 6 & 8. Test implementations (TFIDF, CNN, LSTM)\n",
    "Download all the SemEval datasets from: https://github.com/stevenzim/lrec-2018/tree/master/resources/sem_eval_tweets/SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5656499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Set up the SemEval datasets\n",
    "from helper import *\n",
    "\n",
    "CONSUMER_KEY = 'tSB2RDbxb4U1IuNQa1qLzaErM'\n",
    "CONSUMER_SECRET= 'x9KEdzMELJylzqtswOLRvEn94JkxJFmOIzLwr8C9GvichYrNrD'\n",
    "OAUTH_TOKEN = '757309138443628544-mz7JUvTR4x1p4iCMwj33Y9Xqgt1onwD'\n",
    "OAUTH_TOKEN_SECRET = 'xT93QanDPRZUOU3dewV2RqnQ65NfBNmBhGR0P0bosMJSC'\n",
    "# twitter authentication for tweepy\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "<-Uncomment if building the datasets\n",
    "#Fetch tweets for semeval datasets\n",
    "semevals = ['SemDev.json', 'SemTest2013.json', 'SemTest2014.json', 'SemTest2015.json', 'SemTrain.json']\n",
    "for i in semevals:\n",
    "    semeval = helper.load_json_from_file(i)\n",
    "    for j in range(1, len(semeval)):\n",
    "        try:\n",
    "            ids = semeval[j]['tweet_id']\n",
    "            text = api.get_status(ids).text\n",
    "            semeval[j]['text'] = text\n",
    "        except tweepy.errors.NotFound as e:\n",
    "            print(e)\n",
    "        except tweepy.errors.Forbidden as e:\n",
    "            print(e)\n",
    "        except tweepy.errors.TooManyRequests as e:\n",
    "            print(e)\n",
    "            time.sleep(900)\n",
    "    helper.dump_json_to_file(i, semeval)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d521ac",
   "metadata": {},
   "source": [
    "IT WILL TAKE A LONG TIME (SEVERAL HOURS) TO FETCH ALL THE TWEETS!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10406cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Remove entries that have 'text'=None\n",
    "entries = ['SemDev.json', 'SemTest2013.json', 'SemTest2014.json', 'SemTest2015.json', 'SemTrain.json']\n",
    "\n",
    "for entry in entries:\n",
    "    datafile = helper.load_json_from_file(entry)\n",
    "    # using list comprehension \n",
    "    # to delete dictionary in list\n",
    "    for i in range(len(datafile)-1, -1, -1):\n",
    "        if datafile[i]['text'] == None:\n",
    "            del datafile[i]\n",
    "    helper.dump_json_to_file(entry, datafile)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6af3b5",
   "metadata": {},
   "source": [
    "The following code was provided to us by Saroar Jahan. Remember to download the pre-trained word embeddings vector called: wiki-news-300d-1M.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bf921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from numpy.random import RandomState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Divide csv file into 70% training data and 30% testing data (Hasoc dataset only)\n",
    "#Uncomment for Hasoc dataset\n",
    "#df = pd.read_csv('datasets/Hasoc_2021_english_dataset.csv', usecols=[2,3])\n",
    "#rng = RandomState()\n",
    "#train_data = df.sample(frac=0.7, random_state=rng)\n",
    "#test_data = df.loc[~df.index.isin(train_data.index)]\n",
    "\n",
    "#Uncomment for Semeval dataset\n",
    "test_data = pd.read_csv('SemTest2015.csv', usecols=[4,7])\n",
    "train_data = pd.read_csv('SemTrain.csv', usecols=[1,4])\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "#Uncomment for Hasoc dataset\n",
    "#test_data.replace(to_replace = 'NOT', value = 1, inplace = True)\n",
    "#test_data.replace(to_replace ='HOF', value = 0, inplace = True)\n",
    "#train_data.replace(to_replace ='NOT', value = 1, inplace = True)\n",
    "#train_data.replace(to_replace ='HOF', value = 0, inplace = True)\n",
    "\n",
    "#Uncomment for Semeval dataset\n",
    "test_data['sentiment_num'].replace(to_replace = 0, value = 1, inplace = True)\n",
    "test_data['sentiment_num'].replace(to_replace =-1, value = 0, inplace = True)\n",
    "train_data['sentiment_num'].replace(to_replace = 0, value = 1, inplace = True)\n",
    "train_data['sentiment_num'].replace(to_replace =-1, value = 0, inplace = True)\n",
    "test_data.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x= train_data['text']\n",
    "valid_x= test_data['text']\n",
    "\n",
    "#Uncomment for Hasoc dataset\n",
    "#train_y= train_data['task_1']\n",
    "#valid_y= test_data['task_1']\n",
    "\n",
    "#Uncomment for Semeval dataset\n",
    "train_y= train_data['sentiment_num']\n",
    "valid_y= test_data['sentiment_num']\n",
    "\n",
    "comment=train_x\n",
    "\n",
    "print('train comments length: ',len(train_x))\n",
    "print('test comments length: ',len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(comment)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(comment)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f54acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(comment)\n",
    "word_index = token.word_index\n",
    "    \n",
    "# convert text to sequence of tokens and pad them to ensure e\n",
    "# qual length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, xtrain, ytrain, xvalid, yvalid): \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(xvalid)     \n",
    "    accuracy = metrics.accuracy_score(predictions, yvalid)\n",
    "    f1score = metrics.f1_score(yvalid, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5710904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"LR, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0745ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.np_utils.to_categorical(train_y, 3)\n",
    "valid_y_onehot = keras.utils.np_utils.to_categorical(valid_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(xtrain, ytrain, xvalid, yvalid, epochs = 10):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=epochs)\n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1score = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"CNN, Word Embeddings acuuracy accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26884547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 = layers.LSTM(128)(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(3, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=3)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1004b9",
   "metadata": {},
   "source": [
    "# 7. Comparative abusive lang implementation\n",
    "FOR THIS STEP, PLEASE DOWNLOAD COMPARATIVE ABUSIVE LANG CODES FROM: https://github.com/younggns/comparative-abusive-lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98cfca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Preparation of the dataset\n",
    "cal_data = pd.read_csv(\"simpleheuristic.csv\", \";\", index_col=None, header=None)\n",
    "cal_dict = {}\n",
    "\n",
    "for i in range(1, len(cal_data)):\n",
    "    if int(cal_data[4][i]) == 0:\n",
    "        cal_dict[i] = ['normal', str(cal_data[0][i]), '']\n",
    "    else:\n",
    "        cal_dict[i] = ['hateful', str(cal_data[0][i]), '']\n",
    "\n",
    "#crawling the text data\n",
    "with open('crawled_data.pkl', 'wb') as handle:\n",
    "    pickle.dump(cal_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae53737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install requirements\n",
    "!pip3 install -r requirements.txt\n",
    "\n",
    "#run data preprocessing script\n",
    "!python3 data_preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992afe15",
   "metadata": {},
   "source": [
    "PLEASE NOTE THAT LINUX IS REQUIRED TO SUCCESSFULLY RUN THE FOLLOWING SCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1bd0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to model directory\n",
    "\n",
    "# running script with different classifiers for word-level features, ngram range 1,3 and max features 14000\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf NB\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf LR\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf SVM\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf RF\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf GBT\n",
    "\n",
    "# running script with different classifiers for character-level features, ngram range 3,8 and max features 53000\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf NB\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf LR\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf SVM\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf RF\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf GBT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eadcbf",
   "metadata": {},
   "source": [
    "# 9. Testing implementations with different dataset (CONAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conan = pd.read_csv(\"CONAN_EN.csv\", \";\", index_col=None, header=None)\n",
    "\n",
    "# EMPATH CATEGORIZATION FOR CONAN, EVERY MSG IS HATE SPEECH\n",
    "lexicon = Empath()\n",
    "abusive_categories = []\n",
    "\n",
    "# analyze data\n",
    "for i in range(1, len(conan)):\n",
    "    temp = lexicon.analyze(conan[1][i], normalize=True)\n",
    "    for j in temp.items():\n",
    "        if j[1] != 0.0:\n",
    "            if j[0] not in abusive_categories:\n",
    "                abusive_categories.append(j[0])\n",
    "                \n",
    "header = [\"MESSAGE\"] + [\"isAbusive\"] + abusive_categories\n",
    "#empath categorization results into csv file database\n",
    "recordEmpath(\"empath_dataset_conan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fef62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERAL INQUIRER\n",
    "suffixes = ('#1','#2','#3','#4','#5','#6','#7')\n",
    "header = [\"MESSAGE\"] + [\"isAbusive\"]\n",
    "\n",
    "for i in range(2, len(inquirerData.columns)-2):\n",
    "    header = header + [str(inquirerData[i][0])]\n",
    "\n",
    "#Harvard General Inquirer results into csv file database\n",
    "with open('inquirer_dataset_conan.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter = \";\")\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for i in range(1, len(conan)): #MESSAGES\n",
    "        msg = conan[1][i].replace(\"?\",\"\").replace(\"!\",\"\").replace(\",\",\"\").replace(\".\",\"\").replace('\"',\"\").split(' ')\n",
    "        msg_upper = [x.upper() for x in msg]\n",
    "        temp_data = [None] * len(header)\n",
    "        temp_data[0] = conan[1][i]\n",
    "        temp_data[1] = 1\n",
    "        temp_data = loopInquirer()\n",
    "        writer.writerow(temp_data)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "inquirer_data = pd.read_csv(\"inquirer_dataset_conan.csv\", \";\", index_col=None, header=None, low_memory=False)\n",
    "empath_data = pd.read_csv(\"empath_dataset_conan.csv\", \";\", index_col=None, header=None, low_memory=False)\n",
    "\n",
    "abusiveMessagesInquirer = []\n",
    "abusiveMessagesEmpath = []\n",
    "\n",
    "# Loop all inquirer abusive messages\n",
    "for i in range(1, len(inquirer_data)): # Messages\n",
    "    \n",
    "    #Categories that can be related to hate speech: \n",
    "    #Negativ (3),Pain (15), Race(39), Female(42), BodyPt(60)\n",
    "    if inquirer_data[3][i] == \"Negativ\" or inquirer_data[15][i] == \"Pain\" or inquirer_data[39][i] == \"Race\"  or inquirer_data[33][i] == \"Relig\"or (inquirer_data[42][i]  == \"Female\" and inquirer_data[3][i] == \"Negativ\") or (inquirer_data[60][i] == \"BodyPt\" and inquirer_data[3][i] == \"Negativ\"):\n",
    "        abusiveMessagesInquirer.append(1)\n",
    "        continue\n",
    "    else:\n",
    "        abusiveMessagesInquirer.append(0)\n",
    "        \n",
    "# Loop all empath abusive messages\n",
    "for i in range(1, len(empath_data)):\n",
    "    \n",
    "    #Mark as hate speech if the message has no positive_emotion(10) and\n",
    "    #has words from categories that could include hate speech\n",
    "    \n",
    "    #Categories that can be related to hate speech: \n",
    "    #hate (31), aggression (32), kill (33), violence(35), death(44), negative_emotion(45)\n",
    "    #fight(46), war(47), weapon(48), terrorism(57), suffering(62), pain(65), religion(68)\n",
    "    if  float(empath_data[31][i]) > 0.0 or float(empath_data[32][i]) > 0.0 or float(empath_data[33][i]) > 0.0 or float(empath_data[35][i]) > 0.0 or float(empath_data[44][i]) > 0.0 or float(empath_data[45][i]) > 0.0 or float(empath_data[46][i]) > 0.0 or float(empath_data[47][i]) > 0.0 or float(empath_data[48][i]) > 0.0 or float(empath_data[57][i]) > 0.0 or float(empath_data[62][i]) > 0.0 or float(empath_data[65][i]) > 0.0 or float(empath_data[68][i]) > 0.0:\n",
    "        abusiveMessagesEmpath.append(1)\n",
    "    else:\n",
    "        abusiveMessagesEmpath.append(0)\n",
    "        continue\n",
    "        \n",
    "recordResults(\"simpleheuristic_conan.csv\", abusiveMessagesInquirer, abusiveMessagesEmpath)\n",
    "    \n",
    "#ANALYZE\n",
    "analyzeData(\"simpleheuristic_conan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b388081",
   "metadata": {},
   "source": [
    "Testing Comparative abusive lang implementation with the CONAN dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of the CONAN dataset\n",
    "cal_data_conan = pd.read_csv(\"simpleheuristic_conan.csv\", \";\", index_col=None, header=None)\n",
    "cal_dict_conan = {}\n",
    "\n",
    "for i in range(1, len(cal_data_conan)):\n",
    "    if int(cal_data_conan[4][i]) == 0:\n",
    "        cal_dict_conan[i] = ['normal', str(cal_data_conan[0][i]), '']\n",
    "    else:\n",
    "        cal_dict_conan[i] = ['hateful', str(cal_data_conan[0][i]), '']\n",
    "\n",
    "#crawling the text data\n",
    "with open('crawled_data.pkl', 'wb') as handle:\n",
    "    pickle.dump(cal_dict_conan, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0de786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run data preprocessing script\n",
    "!python3 data_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to model directory\n",
    "\n",
    "# running script with different classifiers for word-level features, ngram range 1,3 and max features 14000\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf NB\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf LR\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf SVM\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf RF\n",
    "!python3 train_ml_models.py --feature_level \"word\" --clf GBT\n",
    "\n",
    "# running script with different classifiers for character-level features, ngram range 3,8 and max features 53000\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf NB\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf LR\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf SVM\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf RF\n",
    "!python3 train_ml_models.py --feature_level \"char\" --clf GBT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1fbbe7",
   "metadata": {},
   "source": [
    "# 10. Suggesting a GUI to exemplify above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fbde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = Tk()\n",
    "\n",
    "win.geometry(\"750x250\")\n",
    "complete_acc_conan = analyzeData(\"simpleheuristic_conan.csv\")\n",
    "complete_acc_askfm = analyzeData(\"simpleheuristic.csv\")\n",
    "\n",
    "def callback():\n",
    "    Label(win, text=\"Complete accuracy of the Conan dataset: \" + complete_acc_conan, font=('Century 20 bold')).pack(pady=4, anchor = W)\n",
    "    Label(win, text= \"Complete accuracy of the AskFm dataset: \" + complete_acc_askfm, font=('Century 20 bold')).pack(pady=4, anchor = W)\n",
    "\n",
    "btn=Button(win, text=\"Show complete accuracy of the datasets\", command= callback)\n",
    "btn.pack(anchor = 'center', padx = 0)\n",
    "\n",
    "win.bind('<Return>',lambda event:callback())\n",
    "win.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd1c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
